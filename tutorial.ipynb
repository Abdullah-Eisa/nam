{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "\n",
    "from nam.config import defaults\n",
    "from nam.types import Config\n",
    "from nam.utils.args import parse_args\n",
    "from nam.data import NAMDataset\n",
    "from nam.models import DNN, FeatureNN, NAM, get_num_units\n",
    "from nam.engine import Engine\n",
    "\n",
    "from main import get_config\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = get_config()\n",
    "pl.seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(device='cpu',\n",
       "          output_dir='output',\n",
       "          training_epochs=10,\n",
       "          lr=0.01,\n",
       "          batch_size=1024,\n",
       "          dropout=0.5,\n",
       "          feature_dropout=0.0,\n",
       "          decay_rate=0.995,\n",
       "          l2_regularization=0.0,\n",
       "          output_regularization=0.0,\n",
       "          num_basis_functions=1000,\n",
       "          units_multiplier=2,\n",
       "          num_units=64,\n",
       "          data_split=1,\n",
       "          seed=1,\n",
       "          cross_val=False,\n",
       "          n_models=1,\n",
       "          num_splits=3,\n",
       "          fold_num=1,\n",
       "          activation='exu',\n",
       "          shuffle=True,\n",
       "          regression=False,\n",
       "          debug=False,\n",
       "          shallow=False,\n",
       "          use_dnn=False,\n",
       "          patience=10,\n",
       "          n_folds=5,\n",
       "          num_workers=16,\n",
       "          learning_rate=0.01,\n",
       "          dataset_name='Teleco',\n",
       "          max_checkpoints_to_keep=1,\n",
       "          save_checkpoint_every_n_epochs=10,\n",
       "          early_stopping_epochs=60)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_columns = [\"income_2\", \"WP1219\", \"WP1220\", \"weo_gdpc_con_ppp\"]\n",
    "targets_column = [\"WP16\"]\n",
    "weights_column = [\"wgt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/GALLUP.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WP16                    0\n",
      "wgt                     0\n",
      "country                 0\n",
      "income_2                0\n",
      "WP1219                  1\n",
      "WP1220                185\n",
      "year                    0\n",
      "weo_gdpc_con_ppp    24122\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing = data.isnull().sum()\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nam.data.base.NAMDataset at 0x7fc6dc9808d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = NAMDataset(config=config,\n",
    "                    csv_file=data, #'data/GALLUP.csv',\n",
    "                    features_columns=features_columns,\n",
    "                    targets_column=targets_column,\n",
    "                    weights_column=weights_column, one_hot=False)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.8043e+04, 1.0000e+00, 1.5000e+01, 3.2658e+04]), tensor([4.]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1206728, 134081)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "  dataset,\n",
    "  [int(np.floor(len(dataset) * .9)),\n",
    "   int(np.ceil(len(dataset) * .1))],\n",
    ")\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(965382, 241346)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "  train_dataset,\n",
    "  [int(np.floor(len(train_dataset) * .8)),\n",
    "   int(np.ceil(len(train_dataset) * .2))],\n",
    ")\n",
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "  train_dataset,\n",
    "  batch_size=config.batch_size,\n",
    "  shuffle=True,\n",
    "  num_workers=config.num_workers,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "  val_dataset,\n",
    "  batch_size=config.batch_size * 5,\n",
    "  shuffle=False,\n",
    "  num_workers=config.num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amrmkayid/anaconda3/envs/nam/lib/python3.7/site-packages/torch/nn/init.py:162: UserWarning: mean is more than 2 std from [a, b] in nn.init.trunc_normal_. The distribution of values may be incorrect.\n",
      "  return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n"
     ]
    }
   ],
   "source": [
    "model = NAM(\n",
    "      config=config,\n",
    "      name=\"NAMModel\",\n",
    "      num_inputs=len(dataset[0][0]),\n",
    "      num_units=get_num_units(config, train_dataloader),\n",
    ").to(device=config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NAM(\n",
       "  (feature_nns): Sequential(\n",
       "    (FeatureNN_0): FeatureNN(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (model): Sequential(\n",
       "        (0): ExU(in_features=1, out_features=1000)\n",
       "        (1): Linear(in_features=1000, out_features=64, bias=True)\n",
       "        (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (3): Linear(in_features=32, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (FeatureNN_1): FeatureNN(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (model): Sequential(\n",
       "        (0): ExU(in_features=1, out_features=4)\n",
       "        (1): Linear(in_features=4, out_features=64, bias=True)\n",
       "        (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (3): Linear(in_features=32, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (FeatureNN_2): FeatureNN(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (model): Sequential(\n",
       "        (0): ExU(in_features=1, out_features=148)\n",
       "        (1): Linear(in_features=148, out_features=64, bias=True)\n",
       "        (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (3): Linear(in_features=32, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (FeatureNN_3): FeatureNN(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (model): Sequential(\n",
       "        (0): ExU(in_features=1, out_features=1000)\n",
       "        (1): Linear(in_features=1000, out_features=64, bias=True)\n",
       "        (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (3): Linear(in_features=32, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = Engine(config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = pl.callbacks.early_stopping.EarlyStopping(\n",
    "   monitor='val_loss',\n",
    "   min_delta=0.00,\n",
    "   patience=config.patience,\n",
    "   verbose=False,\n",
    "   mode='max'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(default_root_dir=config.output_dir, \n",
    "                     max_epochs=config.training_epochs, \n",
    "                     callbacks=[early_stop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | NAM  | 148 K \n",
      "-------------------------------\n",
      "148 K     Trainable params\n",
      "0         Non-trainable params\n",
      "148 K     Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 100%|██████████| 2/2 [00:00<00:00,  3.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amrmkayid/anaconda3/envs/nam/lib/python3.7/site-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  95%|█████████▌| 943/991 [00:41<00:02, 22.61it/s, loss=-3.11, v_num=0, val_loss_epoch=0.693, train_loss_step=-3.16]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   2%|▏         | 1/48 [00:00<00:18,  2.50it/s]\u001b[A\n",
      "Epoch 0:  95%|█████████▌| 946/991 [00:42<00:02, 22.38it/s, loss=-3.11, v_num=0, val_loss_epoch=0.693, train_loss_step=-3.16]\n",
      "Validating:   6%|▋         | 3/48 [00:00<00:12,  3.56it/s]\u001b[A\n",
      "Validating:   8%|▊         | 4/48 [00:00<00:10,  4.26it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▌| 949/991 [00:42<00:01, 22.24it/s, loss=-3.11, v_num=0, val_loss_epoch=0.693, train_loss_step=-3.16]\n",
      "Epoch 0:  96%|█████████▌| 952/991 [00:42<00:01, 22.17it/s, loss=-3.11, v_num=0, val_loss_epoch=0.693, train_loss_step=-3.16]\n",
      "Validating:  19%|█▉        | 9/48 [00:01<00:05,  6.97it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▋| 955/991 [00:43<00:01, 22.09it/s, loss=-3.11, v_num=0, val_loss_epoch=0.693, train_loss_step=-3.16]\n",
      "Validating:  25%|██▌       | 12/48 [00:01<00:04,  8.26it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 958/991 [00:43<00:01, 22.02it/s, loss=-3.11, v_num=0, val_loss_epoch=0.693, train_loss_step=-3.16]\n",
      "Epoch 0:  97%|█████████▋| 961/991 [00:43<00:01, 21.95it/s, loss=-3.11, v_num=0, val_loss_epoch=0.693, train_loss_step=-3.16]\n",
      "Validating:  38%|███▊      | 18/48 [00:02<00:03,  9.99it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 964/991 [00:44<00:01, 21.89it/s, loss=-3.11, v_num=0, val_loss_epoch=0.693, train_loss_step=-3.16]\n",
      "Epoch 0:  98%|█████████▊| 967/991 [00:44<00:01, 21.83it/s, loss=-3.11, v_num=0, val_loss_epoch=0.693, train_loss_step=-3.16]\n",
      "Validating:  50%|█████     | 24/48 [00:02<00:02, 10.94it/s]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 970/991 [00:44<00:00, 21.77it/s, loss=-3.11, v_num=0, val_loss_epoch=0.693, train_loss_step=-3.16]\n",
      "Epoch 0:  98%|█████████▊| 973/991 [00:44<00:00, 21.71it/s, loss=-3.11, v_num=0, val_loss_epoch=0.693, train_loss_step=-3.16]\n",
      "Validating:  62%|██████▎   | 30/48 [00:03<00:01, 11.39it/s]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 976/991 [00:45<00:00, 21.65it/s, loss=-3.11, v_num=0, val_loss_epoch=0.693, train_loss_step=-3.16]\n",
      "Epoch 0:  99%|█████████▉| 979/991 [00:45<00:00, 21.60it/s, loss=-3.11, v_num=0, val_loss_epoch=0.693, train_loss_step=-3.16]\n",
      "Validating:  75%|███████▌  | 36/48 [00:03<00:01, 11.55it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 982/991 [00:45<00:00, 21.54it/s, loss=-3.11, v_num=0, val_loss_epoch=0.693, train_loss_step=-3.16]\n",
      "Epoch 0:  99%|█████████▉| 985/991 [00:45<00:00, 21.48it/s, loss=-3.11, v_num=0, val_loss_epoch=0.693, train_loss_step=-3.16]\n",
      "Validating:  88%|████████▊ | 42/48 [00:04<00:00, 11.62it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 988/991 [00:46<00:00, 21.43it/s, loss=-3.11, v_num=0, val_loss_epoch=0.693, train_loss_step=-3.16]\n",
      "Epoch 0: 100%|██████████| 991/991 [00:46<00:00, 21.37it/s, loss=-3.11, v_num=0, val_loss_epoch=0.693, train_loss_step=-3.16]\n",
      "Epoch 0: 100%|██████████| 991/991 [00:46<00:00, 21.30it/s, loss=-3.11, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.34, val_loss_step=-4.17]\n",
      "Epoch 1:  95%|█████████▌| 943/991 [00:39<00:02, 23.66it/s, loss=-3.45, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.69, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1:  95%|█████████▌| 945/991 [00:40<00:01, 23.45it/s, loss=-3.45, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.69, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Validating:   4%|▍         | 2/48 [00:00<00:15,  3.01it/s]\u001b[A\n",
      "Validating:   6%|▋         | 3/48 [00:00<00:11,  3.80it/s]\u001b[A\n",
      "Epoch 1:  96%|█████████▌| 948/991 [00:40<00:01, 23.35it/s, loss=-3.45, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.69, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Validating:  10%|█         | 5/48 [00:00<00:07,  5.54it/s]\u001b[A\n",
      "Epoch 1:  96%|█████████▌| 951/991 [00:40<00:01, 23.26it/s, loss=-3.45, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.69, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Epoch 1:  96%|█████████▋| 954/991 [00:41<00:01, 23.17it/s, loss=-3.45, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.69, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Validating:  23%|██▎       | 11/48 [00:01<00:04,  8.12it/s]\u001b[A\n",
      "Epoch 1:  97%|█████████▋| 957/991 [00:41<00:01, 23.09it/s, loss=-3.45, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.69, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Epoch 1:  97%|█████████▋| 960/991 [00:41<00:01, 23.00it/s, loss=-3.45, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.69, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Validating:  35%|███▌      | 17/48 [00:01<00:03,  9.53it/s]\u001b[A\n",
      "Epoch 1:  97%|█████████▋| 963/991 [00:42<00:01, 22.92it/s, loss=-3.45, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.69, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Epoch 1:  97%|█████████▋| 966/991 [00:42<00:01, 22.84it/s, loss=-3.45, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.69, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Validating:  48%|████▊     | 23/48 [00:02<00:02, 10.46it/s]\u001b[A\n",
      "Epoch 1:  98%|█████████▊| 969/991 [00:42<00:00, 22.77it/s, loss=-3.45, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.69, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Epoch 1:  98%|█████████▊| 972/991 [00:42<00:00, 22.71it/s, loss=-3.45, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.69, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Validating:  60%|██████    | 29/48 [00:03<00:01, 11.12it/s]\u001b[A\n",
      "Epoch 1:  98%|█████████▊| 975/991 [00:43<00:00, 22.64it/s, loss=-3.45, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.69, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Epoch 1:  99%|█████████▊| 978/991 [00:43<00:00, 22.56it/s, loss=-3.45, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.69, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Validating:  73%|███████▎  | 35/48 [00:03<00:01, 11.16it/s]\u001b[A\n",
      "Epoch 1:  99%|█████████▉| 981/991 [00:43<00:00, 22.49it/s, loss=-3.45, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.69, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Epoch 1:  99%|█████████▉| 984/991 [00:43<00:00, 22.42it/s, loss=-3.45, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.69, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Validating:  85%|████████▌ | 41/48 [00:04<00:00, 10.79it/s]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 987/991 [00:44<00:00, 22.34it/s, loss=-3.45, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.69, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Epoch 1: 100%|█████████▉| 990/991 [00:44<00:00, 22.27it/s, loss=-3.45, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.69, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Epoch 1: 100%|██████████| 991/991 [00:44<00:00, 22.20it/s, loss=-3.45, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.79, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Epoch 2:  95%|█████████▌| 943/991 [00:42<00:02, 22.00it/s, loss=-3.68, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.72, val_loss_step=-4.17, train_loss_epoch=-3.32]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2:  95%|█████████▌| 945/991 [00:43<00:02, 21.83it/s, loss=-3.68, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.72, val_loss_step=-4.17, train_loss_epoch=-3.32]\n",
      "Epoch 2:  96%|█████████▌| 948/991 [00:43<00:01, 21.77it/s, loss=-3.68, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.72, val_loss_step=-4.17, train_loss_epoch=-3.32]\n",
      "Validating:  10%|█         | 5/48 [00:00<00:10,  4.08it/s]\u001b[A\n",
      "Epoch 2:  96%|█████████▌| 951/991 [00:43<00:01, 21.70it/s, loss=-3.68, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.72, val_loss_step=-4.17, train_loss_epoch=-3.32]\n",
      "Epoch 2:  96%|█████████▋| 954/991 [00:44<00:01, 21.64it/s, loss=-3.68, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.72, val_loss_step=-4.17, train_loss_epoch=-3.32]\n",
      "Validating:  23%|██▎       | 11/48 [00:01<00:05,  7.02it/s]\u001b[A\n",
      "Epoch 2:  97%|█████████▋| 957/991 [00:44<00:01, 21.58it/s, loss=-3.68, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.72, val_loss_step=-4.17, train_loss_epoch=-3.32]\n",
      "Epoch 2:  97%|█████████▋| 960/991 [00:44<00:01, 21.52it/s, loss=-3.68, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.72, val_loss_step=-4.17, train_loss_epoch=-3.32]\n",
      "Validating:  35%|███▌      | 17/48 [00:01<00:03,  9.52it/s]\u001b[A\n",
      "Epoch 2:  97%|█████████▋| 963/991 [00:44<00:01, 21.46it/s, loss=-3.68, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.72, val_loss_step=-4.17, train_loss_epoch=-3.32]\n",
      "Epoch 2:  97%|█████████▋| 966/991 [00:45<00:01, 21.40it/s, loss=-3.68, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.72, val_loss_step=-4.17, train_loss_epoch=-3.32]\n",
      "Validating:  48%|████▊     | 23/48 [00:02<00:02, 10.53it/s]\u001b[A\n",
      "Epoch 2:  98%|█████████▊| 969/991 [00:45<00:01, 21.34it/s, loss=-3.68, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.72, val_loss_step=-4.17, train_loss_epoch=-3.32]\n",
      "Epoch 2:  98%|█████████▊| 972/991 [00:45<00:00, 21.29it/s, loss=-3.68, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.72, val_loss_step=-4.17, train_loss_epoch=-3.32]\n",
      "Validating:  60%|██████    | 29/48 [00:02<00:01, 11.27it/s]\u001b[A\n",
      "Epoch 2:  98%|█████████▊| 975/991 [00:45<00:00, 21.24it/s, loss=-3.68, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.72, val_loss_step=-4.17, train_loss_epoch=-3.32]\n",
      "Epoch 2:  99%|█████████▊| 978/991 [00:46<00:00, 21.18it/s, loss=-3.68, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.72, val_loss_step=-4.17, train_loss_epoch=-3.32]\n",
      "Validating:  73%|███████▎  | 35/48 [00:03<00:01, 11.56it/s]\u001b[A\n",
      "Epoch 2:  99%|█████████▉| 981/991 [00:46<00:00, 21.13it/s, loss=-3.68, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.72, val_loss_step=-4.17, train_loss_epoch=-3.32]\n",
      "Epoch 2:  99%|█████████▉| 984/991 [00:46<00:00, 21.08it/s, loss=-3.68, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.72, val_loss_step=-4.17, train_loss_epoch=-3.32]\n",
      "Validating:  85%|████████▌ | 41/48 [00:03<00:00, 11.54it/s]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 987/991 [00:46<00:00, 21.03it/s, loss=-3.68, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.72, val_loss_step=-4.17, train_loss_epoch=-3.32]\n",
      "Epoch 2: 100%|█████████▉| 990/991 [00:47<00:00, 20.98it/s, loss=-3.68, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.72, val_loss_step=-4.17, train_loss_epoch=-3.32]\n",
      "Epoch 2: 100%|██████████| 991/991 [00:47<00:00, 20.92it/s, loss=-3.68, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.74, val_loss_step=-4.17, train_loss_epoch=-3.32]\n",
      "Epoch 3:  95%|█████████▌| 943/991 [00:37<00:01, 25.26it/s, loss=-3.24, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.05, val_loss_step=-4.17, train_loss_epoch=-3.1] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3:  95%|█████████▌| 945/991 [00:37<00:01, 25.05it/s, loss=-3.24, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.05, val_loss_step=-4.17, train_loss_epoch=-3.1]\n",
      "Validating:   6%|▋         | 3/48 [00:00<00:12,  3.60it/s]\u001b[A\n",
      "Epoch 3:  96%|█████████▌| 949/991 [00:37<00:01, 25.00it/s, loss=-3.24, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.05, val_loss_step=-4.17, train_loss_epoch=-3.1]\n",
      "Validating:  15%|█▍        | 7/48 [00:00<00:06,  6.02it/s]\u001b[A\n",
      "Epoch 3:  96%|█████████▌| 953/991 [00:38<00:01, 24.95it/s, loss=-3.24, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.05, val_loss_step=-4.17, train_loss_epoch=-3.1]\n",
      "Validating:  23%|██▎       | 11/48 [00:00<00:04,  9.08it/s]\u001b[A\n",
      "Epoch 3:  97%|█████████▋| 957/991 [00:38<00:01, 24.91it/s, loss=-3.24, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.05, val_loss_step=-4.17, train_loss_epoch=-3.1]\n",
      "Validating:  31%|███▏      | 15/48 [00:01<00:02, 12.04it/s]\u001b[A\n",
      "Epoch 3:  97%|█████████▋| 961/991 [00:38<00:01, 24.87it/s, loss=-3.24, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.05, val_loss_step=-4.17, train_loss_epoch=-3.1]\n",
      "Epoch 3:  97%|█████████▋| 965/991 [00:38<00:01, 24.85it/s, loss=-3.24, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.05, val_loss_step=-4.17, train_loss_epoch=-3.1]\n",
      "Validating:  46%|████▌     | 22/48 [00:01<00:01, 16.27it/s]\u001b[A\n",
      "Epoch 3:  98%|█████████▊| 969/991 [00:39<00:00, 24.83it/s, loss=-3.24, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.05, val_loss_step=-4.17, train_loss_epoch=-3.1]\n",
      "Epoch 3:  98%|█████████▊| 973/991 [00:39<00:00, 24.81it/s, loss=-3.24, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.05, val_loss_step=-4.17, train_loss_epoch=-3.1]\n",
      "Epoch 3:  99%|█████████▊| 977/991 [00:39<00:00, 24.80it/s, loss=-3.24, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.05, val_loss_step=-4.17, train_loss_epoch=-3.1]\n",
      "Validating:  71%|███████   | 34/48 [00:02<00:00, 19.99it/s]\u001b[A\n",
      "Epoch 3:  99%|█████████▉| 981/991 [00:39<00:00, 24.79it/s, loss=-3.24, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.05, val_loss_step=-4.17, train_loss_epoch=-3.1]\n",
      "Epoch 3:  99%|█████████▉| 985/991 [00:39<00:00, 24.77it/s, loss=-3.24, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.05, val_loss_step=-4.17, train_loss_epoch=-3.1]\n",
      "Epoch 3: 100%|█████████▉| 989/991 [00:39<00:00, 24.75it/s, loss=-3.24, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.05, val_loss_step=-4.17, train_loss_epoch=-3.1]\n",
      "Epoch 3: 100%|██████████| 991/991 [00:40<00:00, 24.68it/s, loss=-3.24, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.07, val_loss_step=-4.17, train_loss_epoch=-3.1]\n",
      "Epoch 4:  95%|█████████▌| 944/991 [00:37<00:01, 24.90it/s, loss=-3.2, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.04, val_loss_step=-4.17, train_loss_epoch=-3.29] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   2%|▏         | 1/48 [00:00<00:16,  2.77it/s]\u001b[A\n",
      "Validating:   4%|▍         | 2/48 [00:00<00:13,  3.53it/s]\u001b[A\n",
      "Epoch 4:  96%|█████████▌| 948/991 [00:38<00:01, 24.59it/s, loss=-3.2, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.04, val_loss_step=-4.17, train_loss_epoch=-3.29]\n",
      "Validating:  12%|█▎        | 6/48 [00:00<00:07,  5.53it/s]\u001b[A\n",
      "Epoch 4:  96%|█████████▌| 952/991 [00:38<00:01, 24.51it/s, loss=-3.2, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.04, val_loss_step=-4.17, train_loss_epoch=-3.29]\n",
      "Validating:  21%|██        | 10/48 [00:01<00:04,  8.53it/s]\u001b[A\n",
      "Epoch 4:  96%|█████████▋| 956/991 [00:39<00:01, 24.48it/s, loss=-3.2, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.04, val_loss_step=-4.17, train_loss_epoch=-3.29]\n",
      "Validating:  29%|██▉       | 14/48 [00:01<00:02, 11.53it/s]\u001b[A\n",
      "Epoch 4:  97%|█████████▋| 960/991 [00:39<00:01, 24.44it/s, loss=-3.2, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.04, val_loss_step=-4.17, train_loss_epoch=-3.29]\n",
      "Validating:  38%|███▊      | 18/48 [00:01<00:02, 14.43it/s]\u001b[A\n",
      "Epoch 4:  97%|█████████▋| 964/991 [00:39<00:01, 24.41it/s, loss=-3.2, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.04, val_loss_step=-4.17, train_loss_epoch=-3.29]\n",
      "Epoch 4:  98%|█████████▊| 968/991 [00:39<00:00, 24.40it/s, loss=-3.2, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.04, val_loss_step=-4.17, train_loss_epoch=-3.29]\n",
      "Epoch 4:  98%|█████████▊| 972/991 [00:39<00:00, 24.39it/s, loss=-3.2, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.04, val_loss_step=-4.17, train_loss_epoch=-3.29]\n",
      "Validating:  60%|██████    | 29/48 [00:01<00:00, 19.12it/s]\u001b[A\n",
      "Epoch 4:  98%|█████████▊| 976/991 [00:40<00:00, 24.38it/s, loss=-3.2, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.04, val_loss_step=-4.17, train_loss_epoch=-3.29]\n",
      "Epoch 4:  99%|█████████▉| 980/991 [00:40<00:00, 24.37it/s, loss=-3.2, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.04, val_loss_step=-4.17, train_loss_epoch=-3.29]\n",
      "Epoch 4:  99%|█████████▉| 984/991 [00:40<00:00, 24.37it/s, loss=-3.2, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.04, val_loss_step=-4.17, train_loss_epoch=-3.29]\n",
      "Validating:  85%|████████▌ | 41/48 [00:02<00:00, 21.91it/s]\u001b[A\n",
      "Epoch 4: 100%|█████████▉| 988/991 [00:40<00:00, 24.36it/s, loss=-3.2, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.04, val_loss_step=-4.17, train_loss_epoch=-3.29]\n",
      "Epoch 4: 100%|██████████| 991/991 [00:40<00:00, 24.26it/s, loss=-3.2, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.15, val_loss_step=-4.17, train_loss_epoch=-3.29]\n",
      "Epoch 5:  95%|█████████▌| 944/991 [00:36<00:01, 25.85it/s, loss=-3.64, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.74, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   2%|▏         | 1/48 [00:01<00:48,  1.04s/it]\u001b[A\n",
      "Epoch 5:  96%|█████████▌| 948/991 [00:37<00:01, 25.09it/s, loss=-3.64, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.74, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Validating:  10%|█         | 5/48 [00:01<00:23,  1.84it/s]\u001b[A\n",
      "Epoch 5:  96%|█████████▌| 952/991 [00:37<00:01, 25.06it/s, loss=-3.64, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.74, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Validating:  19%|█▉        | 9/48 [00:01<00:11,  3.41it/s]\u001b[A\n",
      "Epoch 5:  96%|█████████▋| 956/991 [00:38<00:01, 25.04it/s, loss=-3.64, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.74, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Validating:  29%|██▉       | 14/48 [00:01<00:05,  5.84it/s]\u001b[A\n",
      "Epoch 5:  97%|█████████▋| 960/991 [00:38<00:01, 25.00it/s, loss=-3.64, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.74, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Epoch 5:  97%|█████████▋| 964/991 [00:38<00:01, 24.98it/s, loss=-3.64, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.74, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Epoch 5:  98%|█████████▊| 968/991 [00:38<00:00, 24.97it/s, loss=-3.64, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.74, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Validating:  52%|█████▏    | 25/48 [00:02<00:01, 13.08it/s]\u001b[A\n",
      "Epoch 5:  98%|█████████▊| 972/991 [00:38<00:00, 24.95it/s, loss=-3.64, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.74, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Epoch 5:  98%|█████████▊| 976/991 [00:39<00:00, 24.94it/s, loss=-3.64, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.74, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Epoch 5:  99%|█████████▉| 980/991 [00:39<00:00, 24.93it/s, loss=-3.64, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.74, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Validating:  77%|███████▋  | 37/48 [00:02<00:00, 18.69it/s]\u001b[A\n",
      "Epoch 5:  99%|█████████▉| 984/991 [00:39<00:00, 24.90it/s, loss=-3.64, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.74, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Epoch 5: 100%|█████████▉| 988/991 [00:39<00:00, 24.88it/s, loss=-3.64, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.74, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Epoch 5: 100%|██████████| 991/991 [00:39<00:00, 24.79it/s, loss=-3.64, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.69, val_loss_step=-4.17, train_loss_epoch=-3.4]\n",
      "Epoch 6:  95%|█████████▌| 944/991 [00:36<00:01, 25.59it/s, loss=-3.54, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.29, val_loss_step=-4.17, train_loss_epoch=-3.26]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   2%|▏         | 1/48 [00:00<00:22,  2.09it/s]\u001b[A\n",
      "Epoch 6:  96%|█████████▌| 948/991 [00:37<00:01, 25.23it/s, loss=-3.54, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.29, val_loss_step=-4.17, train_loss_epoch=-3.26]\n",
      "Validating:  10%|█         | 5/48 [00:00<00:11,  3.75it/s]\u001b[A\n",
      "Epoch 6:  96%|█████████▌| 952/991 [00:37<00:01, 25.19it/s, loss=-3.54, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.29, val_loss_step=-4.17, train_loss_epoch=-3.26]\n",
      "Validating:  19%|█▉        | 9/48 [00:00<00:06,  6.26it/s]\u001b[A\n",
      "Epoch 6:  96%|█████████▋| 956/991 [00:38<00:01, 25.14it/s, loss=-3.54, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.29, val_loss_step=-4.17, train_loss_epoch=-3.26]\n",
      "Validating:  27%|██▋       | 13/48 [00:01<00:03,  9.25it/s]\u001b[A\n",
      "Epoch 6:  97%|█████████▋| 960/991 [00:38<00:01, 25.09it/s, loss=-3.54, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.29, val_loss_step=-4.17, train_loss_epoch=-3.26]\n",
      "Validating:  35%|███▌      | 17/48 [00:01<00:02, 12.43it/s]\u001b[A\n",
      "Epoch 6:  97%|█████████▋| 964/991 [00:38<00:01, 25.06it/s, loss=-3.54, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.29, val_loss_step=-4.17, train_loss_epoch=-3.26]\n",
      "Validating:  44%|████▍     | 21/48 [00:01<00:01, 14.95it/s]\u001b[A\n",
      "Epoch 6:  98%|█████████▊| 968/991 [00:38<00:00, 25.02it/s, loss=-3.54, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.29, val_loss_step=-4.17, train_loss_epoch=-3.26]\n",
      "Validating:  52%|█████▏    | 25/48 [00:01<00:01, 16.57it/s]\u001b[A\n",
      "Epoch 6:  98%|█████████▊| 972/991 [00:38<00:00, 24.99it/s, loss=-3.54, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.29, val_loss_step=-4.17, train_loss_epoch=-3.26]\n",
      "Validating:  60%|██████    | 29/48 [00:02<00:01, 17.69it/s]\u001b[A\n",
      "Epoch 6:  98%|█████████▊| 976/991 [00:39<00:00, 24.96it/s, loss=-3.54, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.29, val_loss_step=-4.17, train_loss_epoch=-3.26]\n",
      "Validating:  69%|██████▉   | 33/48 [00:02<00:00, 18.27it/s]\u001b[A\n",
      "Epoch 6:  99%|█████████▉| 980/991 [00:39<00:00, 24.91it/s, loss=-3.54, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.29, val_loss_step=-4.17, train_loss_epoch=-3.26]\n",
      "Validating:  77%|███████▋  | 37/48 [00:02<00:00, 17.40it/s]\u001b[A\n",
      "Epoch 6:  99%|█████████▉| 984/991 [00:39<00:00, 24.87it/s, loss=-3.54, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.29, val_loss_step=-4.17, train_loss_epoch=-3.26]\n",
      "Validating:  85%|████████▌ | 41/48 [00:02<00:00, 17.81it/s]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 988/991 [00:39<00:00, 24.84it/s, loss=-3.54, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.29, val_loss_step=-4.17, train_loss_epoch=-3.26]\n",
      "Validating:  94%|█████████▍| 45/48 [00:02<00:00, 18.30it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 991/991 [00:40<00:00, 24.77it/s, loss=-3.54, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.52, val_loss_step=-4.17, train_loss_epoch=-3.26]\n",
      "Epoch 7:  95%|█████████▌| 944/991 [00:36<00:01, 25.94it/s, loss=-3.35, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.42, val_loss_step=-4.17, train_loss_epoch=-3.55]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   2%|▏         | 1/48 [00:00<00:18,  2.56it/s]\u001b[A\n",
      "Epoch 7:  96%|█████████▌| 948/991 [00:36<00:01, 25.64it/s, loss=-3.35, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.42, val_loss_step=-4.17, train_loss_epoch=-3.55]\n",
      "Validating:  10%|█         | 5/48 [00:00<00:09,  4.51it/s]\u001b[A\n",
      "Epoch 7:  96%|█████████▌| 952/991 [00:37<00:01, 25.60it/s, loss=-3.35, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.42, val_loss_step=-4.17, train_loss_epoch=-3.55]\n",
      "Validating:  19%|█▉        | 9/48 [00:00<00:05,  7.41it/s]\u001b[A\n",
      "Epoch 7:  96%|█████████▋| 956/991 [00:37<00:01, 25.57it/s, loss=-3.35, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.42, val_loss_step=-4.17, train_loss_epoch=-3.55]\n",
      "Epoch 7:  97%|█████████▋| 960/991 [00:37<00:01, 25.55it/s, loss=-3.35, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.42, val_loss_step=-4.17, train_loss_epoch=-3.55]\n",
      "Validating:  35%|███▌      | 17/48 [00:01<00:02, 12.84it/s]\u001b[A\n",
      "Epoch 7:  97%|█████████▋| 964/991 [00:37<00:01, 25.53it/s, loss=-3.35, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.42, val_loss_step=-4.17, train_loss_epoch=-3.55]\n",
      "Epoch 7:  98%|█████████▊| 968/991 [00:37<00:00, 25.52it/s, loss=-3.35, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.42, val_loss_step=-4.17, train_loss_epoch=-3.55]\n",
      "Epoch 7:  98%|█████████▊| 972/991 [00:38<00:00, 25.50it/s, loss=-3.35, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.42, val_loss_step=-4.17, train_loss_epoch=-3.55]\n",
      "Validating:  60%|██████    | 29/48 [00:01<00:01, 18.61it/s]\u001b[A\n",
      "Epoch 7:  98%|█████████▊| 976/991 [00:38<00:00, 25.48it/s, loss=-3.35, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.42, val_loss_step=-4.17, train_loss_epoch=-3.55]\n",
      "Epoch 7:  99%|█████████▉| 980/991 [00:38<00:00, 25.46it/s, loss=-3.35, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.42, val_loss_step=-4.17, train_loss_epoch=-3.55]\n",
      "Epoch 7:  99%|█████████▉| 984/991 [00:38<00:00, 25.44it/s, loss=-3.35, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.42, val_loss_step=-4.17, train_loss_epoch=-3.55]\n",
      "Validating:  85%|████████▌ | 41/48 [00:02<00:00, 20.74it/s]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 988/991 [00:38<00:00, 25.43it/s, loss=-3.35, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.42, val_loss_step=-4.17, train_loss_epoch=-3.55]\n",
      "Epoch 7: 100%|██████████| 991/991 [00:39<00:00, 25.36it/s, loss=-3.35, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.7, val_loss_step=-4.17, train_loss_epoch=-3.55] \n",
      "Epoch 8:  95%|█████████▌| 944/991 [00:34<00:01, 27.19it/s, loss=-3.79, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.78, val_loss_step=-4.17, train_loss_epoch=-3.33]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   2%|▏         | 1/48 [00:00<00:17,  2.63it/s]\u001b[A\n",
      "Epoch 8:  96%|█████████▌| 948/991 [00:35<00:01, 26.87it/s, loss=-3.79, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.78, val_loss_step=-4.17, train_loss_epoch=-3.33]\n",
      "Validating:  10%|█         | 5/48 [00:00<00:09,  4.61it/s]\u001b[A\n",
      "Epoch 8:  96%|█████████▌| 952/991 [00:35<00:01, 26.81it/s, loss=-3.79, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.78, val_loss_step=-4.17, train_loss_epoch=-3.33]\n",
      "Validating:  19%|█▉        | 9/48 [00:00<00:05,  7.48it/s]\u001b[A\n",
      "Epoch 8:  96%|█████████▋| 956/991 [00:35<00:01, 26.76it/s, loss=-3.79, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.78, val_loss_step=-4.17, train_loss_epoch=-3.33]\n",
      "Validating:  27%|██▋       | 13/48 [00:01<00:03, 10.62it/s]\u001b[A\n",
      "Epoch 8:  97%|█████████▋| 960/991 [00:35<00:01, 26.70it/s, loss=-3.79, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.78, val_loss_step=-4.17, train_loss_epoch=-3.33]\n",
      "Validating:  35%|███▌      | 17/48 [00:01<00:02, 13.55it/s]\u001b[A\n",
      "Epoch 8:  97%|█████████▋| 964/991 [00:36<00:01, 26.67it/s, loss=-3.79, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.78, val_loss_step=-4.17, train_loss_epoch=-3.33]\n",
      "Epoch 8:  98%|█████████▊| 968/991 [00:36<00:00, 26.65it/s, loss=-3.79, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.78, val_loss_step=-4.17, train_loss_epoch=-3.33]\n",
      "Epoch 8:  98%|█████████▊| 972/991 [00:36<00:00, 26.64it/s, loss=-3.79, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.78, val_loss_step=-4.17, train_loss_epoch=-3.33]\n",
      "Validating:  60%|██████    | 29/48 [00:01<00:00, 19.75it/s]\u001b[A\n",
      "Epoch 8:  98%|█████████▊| 976/991 [00:36<00:00, 26.62it/s, loss=-3.79, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.78, val_loss_step=-4.17, train_loss_epoch=-3.33]\n",
      "Epoch 8:  99%|█████████▉| 980/991 [00:36<00:00, 26.60it/s, loss=-3.79, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.78, val_loss_step=-4.17, train_loss_epoch=-3.33]\n",
      "Epoch 8:  99%|█████████▉| 984/991 [00:37<00:00, 26.58it/s, loss=-3.79, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.78, val_loss_step=-4.17, train_loss_epoch=-3.33]\n",
      "Validating:  85%|████████▌ | 41/48 [00:02<00:00, 21.68it/s]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 988/991 [00:37<00:00, 26.56it/s, loss=-3.79, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.78, val_loss_step=-4.17, train_loss_epoch=-3.33]\n",
      "Epoch 8: 100%|██████████| 991/991 [00:37<00:00, 26.46it/s, loss=-3.79, v_num=0, val_loss_epoch=-4.21, train_loss_step=-3.39, val_loss_step=-4.17, train_loss_epoch=-3.33]\n",
      "Epoch 9:  95%|█████████▌| 944/991 [00:36<00:01, 25.97it/s, loss=-2.84, v_num=0, val_loss_epoch=-4.21, train_loss_step=-2.99, val_loss_step=-4.17, train_loss_epoch=-3.51]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   2%|▏         | 1/48 [00:00<00:18,  2.51it/s]\u001b[A\n",
      "Validating:   4%|▍         | 2/48 [00:00<00:17,  2.57it/s]\u001b[A\n",
      "Epoch 9:  96%|█████████▌| 948/991 [00:37<00:01, 25.44it/s, loss=-2.84, v_num=0, val_loss_epoch=-4.21, train_loss_step=-2.99, val_loss_step=-4.17, train_loss_epoch=-3.51]\n",
      "Epoch 9:  96%|█████████▌| 952/991 [00:37<00:01, 25.41it/s, loss=-2.84, v_num=0, val_loss_epoch=-4.21, train_loss_step=-2.99, val_loss_step=-4.17, train_loss_epoch=-3.51]\n",
      "Validating:  19%|█▉        | 9/48 [00:01<00:06,  5.92it/s]\u001b[A\n",
      "Epoch 9:  96%|█████████▋| 956/991 [00:37<00:01, 25.39it/s, loss=-2.84, v_num=0, val_loss_epoch=-4.21, train_loss_step=-2.99, val_loss_step=-4.17, train_loss_epoch=-3.51]\n",
      "Epoch 9:  97%|█████████▋| 960/991 [00:37<00:01, 25.36it/s, loss=-2.84, v_num=0, val_loss_epoch=-4.21, train_loss_step=-2.99, val_loss_step=-4.17, train_loss_epoch=-3.51]\n",
      "Validating:  35%|███▌      | 17/48 [00:01<00:02, 11.01it/s]\u001b[A\n",
      "Epoch 9:  97%|█████████▋| 964/991 [00:38<00:01, 25.34it/s, loss=-2.84, v_num=0, val_loss_epoch=-4.21, train_loss_step=-2.99, val_loss_step=-4.17, train_loss_epoch=-3.51]\n",
      "Epoch 9:  98%|█████████▊| 968/991 [00:38<00:00, 25.33it/s, loss=-2.84, v_num=0, val_loss_epoch=-4.21, train_loss_step=-2.99, val_loss_step=-4.17, train_loss_epoch=-3.51]\n",
      "Epoch 9:  98%|█████████▊| 972/991 [00:38<00:00, 25.31it/s, loss=-2.84, v_num=0, val_loss_epoch=-4.21, train_loss_step=-2.99, val_loss_step=-4.17, train_loss_epoch=-3.51]\n",
      "Validating:  60%|██████    | 29/48 [00:02<00:01, 17.94it/s]\u001b[A\n",
      "Epoch 9:  98%|█████████▊| 976/991 [00:38<00:00, 25.30it/s, loss=-2.84, v_num=0, val_loss_epoch=-4.21, train_loss_step=-2.99, val_loss_step=-4.17, train_loss_epoch=-3.51]\n",
      "Epoch 9:  99%|█████████▉| 980/991 [00:38<00:00, 25.28it/s, loss=-2.84, v_num=0, val_loss_epoch=-4.21, train_loss_step=-2.99, val_loss_step=-4.17, train_loss_epoch=-3.51]\n",
      "Epoch 9:  99%|█████████▉| 984/991 [00:38<00:00, 25.26it/s, loss=-2.84, v_num=0, val_loss_epoch=-4.21, train_loss_step=-2.99, val_loss_step=-4.17, train_loss_epoch=-3.51]\n",
      "Validating:  85%|████████▌ | 41/48 [00:02<00:00, 20.72it/s]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 988/991 [00:39<00:00, 25.25it/s, loss=-2.84, v_num=0, val_loss_epoch=-4.21, train_loss_step=-2.99, val_loss_step=-4.17, train_loss_epoch=-3.51]\n",
      "Epoch 9: 100%|██████████| 991/991 [00:39<00:00, 25.16it/s, loss=-2.84, v_num=0, val_loss_epoch=-4.21, train_loss_step=-2.48, val_loss_step=-4.17, train_loss_epoch=-3.51]\n",
      "Epoch 9: 100%|██████████| 991/991 [00:39<00:00, 25.09it/s, loss=-2.84, v_num=0, val_loss_epoch=-4.21, train_loss_step=-2.48, val_loss_step=-4.17, train_loss_epoch=-3.51]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(engine, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datalloader = DataLoader(\n",
    "  test_dataset,\n",
    "  batch_size=config.batch_size,\n",
    "  shuffle=False,\n",
    "  num_workers=config.num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 131/131 [00:01<00:00, 75.47it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': tensor(-4.1810),\n",
      " 'test_loss_epoch': tensor(-4.2002),\n",
      " 'val_loss': tensor(-4.1669),\n",
      " 'val_loss_epoch': tensor(-4.2112)}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss_epoch': -4.21120023727417,\n",
       "  'val_loss': -4.166907787322998,\n",
       "  'test_loss_epoch': -4.200181007385254,\n",
       "  'test_loss': -4.181014537811279}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(engine, test_datalloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: OLD K-fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GALLUP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_columns = [\"income_2\", \"WP1219\", \"WP1220\", \"weo_gdpc_con_ppp\"]\n",
    "targets_column = [\"WP16\"]\n",
    "weights_column = [\"wgt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/GALLUP.csv')\n",
    "# data = data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WP16                    0\n",
      "wgt                     0\n",
      "country                 0\n",
      "income_2                0\n",
      "WP1219                  1\n",
      "WP1220                185\n",
      "year                    0\n",
      "weo_gdpc_con_ppp    24122\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/GALLUP.csv')\n",
    "# data = data[:1000]\n",
    "missing = data.isnull().sum()\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nam.data.base.NAMDataset at 0x7fe4b13f3e50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = NAMDataset(config=config,\n",
    "                    csv_file=data, #'data/GALLUP.csv',\n",
    "                    features_columns=features_columns,\n",
    "                    targets_column=targets_column,\n",
    "                    weights_column=weights_column, one_hot=False)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders = dataset.data_loaders(\n",
    "      n_splits=config.num_splits,\n",
    "      batch_size=config.batch_size,\n",
    "      shuffle=config.shuffle,\n",
    "      stratified=not config.regression,\n",
    "      random_state=config.seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold((1,)), train: 938566, test: 134081\n",
      "[tensor([[76845,     1,    40,   605],\n",
      "        [80149,     0,    43,   861],\n",
      "        [    0,     1,     4,   677],\n",
      "        ...,\n",
      "        [99347,     1,    44,   965],\n",
      "        [79297,     0,    46,   940],\n",
      "        [85594,     0,    24,   915]]), tensor([[  8883],\n",
      "        [106160],\n",
      "        [146743],\n",
      "        ...,\n",
      "        [ 12864],\n",
      "        [ 77549],\n",
      "        [  6516]]), tensor([[  8],\n",
      "        [100],\n",
      "        [ 62],\n",
      "        ...,\n",
      "        [ 50],\n",
      "        [135],\n",
      "        [ 68]])] [tensor([[ 30686,      0,     31,    389],\n",
      "        [103458,      1,     47,   1040],\n",
      "        [ 41558,      0,      5,    133],\n",
      "        ...,\n",
      "        [ 71748,      1,      6,    442],\n",
      "        [ 75880,      0,     48,    965],\n",
      "        [ 65557,      1,     20,    758]]), tensor([[ 54025],\n",
      "        [233171],\n",
      "        [ 57097],\n",
      "        ...,\n",
      "        [105269],\n",
      "        [ 35618],\n",
      "        [251079]]), tensor([[ 61],\n",
      "        [133],\n",
      "        [119],\n",
      "        ...,\n",
      "        [ 17],\n",
      "        [ 50],\n",
      "        [ 58]])]\n",
      "Fold((2,)), train: 938566, test: 134081\n",
      "[tensor([[57963,     0,    15,   595],\n",
      "        [41996,     1,    29,   670],\n",
      "        [57557,     1,     7,   673],\n",
      "        ...,\n",
      "        [21866,     1,    11,   294],\n",
      "        [86208,     0,    38,   938],\n",
      "        [56766,     0,    46,   599]]), tensor([[188507],\n",
      "        [ 94770],\n",
      "        [245095],\n",
      "        ...,\n",
      "        [ 74393],\n",
      "        [  2710],\n",
      "        [  7186]]), tensor([[ 19],\n",
      "        [148],\n",
      "        [ 90],\n",
      "        ...,\n",
      "        [ 60],\n",
      "        [ 12],\n",
      "        [ 18]])] [tensor([[50339,     0,    51,   588],\n",
      "        [25893,     0,    26,   278],\n",
      "        [67942,     1,    26,   879],\n",
      "        ...,\n",
      "        [ 8340,     1,    16,     5],\n",
      "        [45157,     0,    41,   327],\n",
      "        [83594,     1,    22,   123]]), tensor([[273835],\n",
      "        [  2800],\n",
      "        [ 22461],\n",
      "        ...,\n",
      "        [113355],\n",
      "        [ 22036],\n",
      "        [  9630]]), tensor([[ 19],\n",
      "        [ 60],\n",
      "        [128],\n",
      "        ...,\n",
      "        [102],\n",
      "        [  3],\n",
      "        [143]])]\n",
      "Fold((3,)), train: 938566, test: 134081\n",
      "[tensor([[81836,     0,    41,   815],\n",
      "        [44682,     0,     3,   466],\n",
      "        [47397,     1,    19,   448],\n",
      "        ...,\n",
      "        [    0,     1,    44,   705],\n",
      "        [54919,     0,     9,   737],\n",
      "        [71725,     0,    21,   814]]), tensor([[162613],\n",
      "        [191220],\n",
      "        [123983],\n",
      "        ...,\n",
      "        [ 32838],\n",
      "        [153658],\n",
      "        [ 95362]]), tensor([[ 38],\n",
      "        [ 42],\n",
      "        [140],\n",
      "        ...,\n",
      "        [ 75],\n",
      "        [ 28],\n",
      "        [123]])] [tensor([[82345,     0,     9,  1073],\n",
      "        [87099,     0,    18,   870],\n",
      "        [50335,     0,     4,   640],\n",
      "        ...,\n",
      "        [95459,     1,    25,  1062],\n",
      "        [43549,     1,     7,   250],\n",
      "        [60123,     0,    29,   739]]), tensor([[273360],\n",
      "        [220001],\n",
      "        [190369],\n",
      "        ...,\n",
      "        [175191],\n",
      "        [ 92733],\n",
      "        [200956]]), tensor([[122],\n",
      "        [100],\n",
      "        [ 90],\n",
      "        ...,\n",
      "        [104],\n",
      "        [ 91],\n",
      "        [107]])]\n",
      "Fold((4,)), train: 938566, test: 134081\n",
      "[tensor([[47694,     0,    24,   323],\n",
      "        [85526,     1,    14,   986],\n",
      "        [55622,     0,    34,   681],\n",
      "        ...,\n",
      "        [92618,     1,    13,   768],\n",
      "        [51334,     0,    31,   294],\n",
      "        [94395,     0,    19,  1077]]), tensor([[230215],\n",
      "        [193861],\n",
      "        [158689],\n",
      "        ...,\n",
      "        [ 57085],\n",
      "        [266262],\n",
      "        [118786]]), tensor([[ 16],\n",
      "        [  7],\n",
      "        [ 28],\n",
      "        ...,\n",
      "        [111],\n",
      "        [ 60],\n",
      "        [ 81]])] [tensor([[33652,     0,     3, 21226],\n",
      "        [    0,     0,    32,   113],\n",
      "        [30393,     1,    21,   252],\n",
      "        ...,\n",
      "        [24116,     1,    28,   457],\n",
      "        [92596,     1,    42,  1040],\n",
      "        [83576,     0,    31,   772]]), tensor([[186874],\n",
      "        [252142],\n",
      "        [180010],\n",
      "        ...,\n",
      "        [193926],\n",
      "        [124294],\n",
      "        [252785]]), tensor([[ 32],\n",
      "        [153],\n",
      "        [106],\n",
      "        ...,\n",
      "        [ 42],\n",
      "        [133],\n",
      "        [ 85]])]\n",
      "Fold((5,)), train: 938566, test: 134081\n",
      "[tensor([[28754,     0,    21,   570],\n",
      "        [39911,     0,    22,   741],\n",
      "        [ 7578,     1,    20,   197],\n",
      "        ...,\n",
      "        [24036,     0,     6,   321],\n",
      "        [ 2869,     0,     6,  1060],\n",
      "        [89512,     0,     5,   985]]), tensor([[ 10087],\n",
      "        [ 68334],\n",
      "        [194333],\n",
      "        ...,\n",
      "        [ 41203],\n",
      "        [221819],\n",
      "        [272397]]), tensor([[138],\n",
      "        [ 75],\n",
      "        [ 24],\n",
      "        ...,\n",
      "        [ 60],\n",
      "        [104],\n",
      "        [118]])] [tensor([[93817,     1,    45,   898],\n",
      "        [90521,     1,    36,   956],\n",
      "        [66342,     0,     6,   718],\n",
      "        ...,\n",
      "        [36675,     1,    39,   144],\n",
      "        [93629,     0,    16,   965],\n",
      "        [67064,     0,    41,  1007]]), tensor([[ 59436],\n",
      "        [125735],\n",
      "        [ 39585],\n",
      "        ...,\n",
      "        [  7085],\n",
      "        [129262],\n",
      "        [ 39975]]), tensor([[146],\n",
      "        [ 25],\n",
      "        [ 36],\n",
      "        ...,\n",
      "        [ 98],\n",
      "        [ 50],\n",
      "        [ 99]])]\n"
     ]
    }
   ],
   "source": [
    "for train_dl, val_dl in dataset.data_loaders():\n",
    "    print(next(iter(train_dl)), next(iter(val_dl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_dl))\n",
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_columns = [\n",
    "    \"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n",
    "     \"population\", \"households\", \"median_income\", #\"total_bedrooms\",\n",
    "]\n",
    "targets_column = [\"median_house_value\"]\n",
    "dataset = NAMDataset(config=config,\n",
    "                    csv_file='data/housing.csv',\n",
    "                    features_columns=features_columns,\n",
    "                    targets_column=targets_column, \n",
    "                    one_hot=False)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_columns = [\n",
    "    \"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n",
    "    \"total_bedrooms\", \"population\", \"households\", \"median_income\"\n",
    "]\n",
    "targets_column = [\"median_house_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nam.data.base.NAMDataset at 0x7feb49642250>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = NAMDataset(config=config,\n",
    "                    csv_file='data/housing.csv',\n",
    "                    features_columns=features_columns,\n",
    "                    targets_column=targets_column, one_hot=False)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold((1,)), train: 16512, test: 4128\n",
      "[tensor([[ 252,  583,   16,  ..., 2395, 1314, 9240],\n",
      "        [ 484,  350,   33,  ...,  753,  296, 5622],\n",
      "        [ 573,  124,   45,  ..., 1518,  586, 1051],\n",
      "        ...,\n",
      "        [ 330,  468,   26,  ..., 1435,  478, 3444],\n",
      "        [ 201,  472,    9,  ..., 2002, 1000, 5214],\n",
      "        [ 271,  570,   19,  ...,  907,  335, 8112]]), tensor([[1379],\n",
      "        [ 328],\n",
      "        [1334],\n",
      "        ...,\n",
      "        [ 276],\n",
      "        [2098],\n",
      "        [1775]])] [tensor([[  689,     4,    19,  ...,  1470,   352,  6979],\n",
      "        [  677,    19,    51,  ...,   846,   533,  1960],\n",
      "        [  631,   110,     1,  ...,  2820,   789, 12705],\n",
      "        ...,\n",
      "        [  563,   146,    39,  ...,   832,   229,   252],\n",
      "        [  178,   468,    14,  ...,   952,   363, 12756],\n",
      "        [  607,   120,     8,  ...,  1230,   164,  5482]]), tensor([[ 905],\n",
      "        [ 710],\n",
      "        [3786],\n",
      "        ...,\n",
      "        [ 930],\n",
      "        [3841],\n",
      "        [1237]])]\n",
      "Fold((2,)), train: 16512, test: 4128\n",
      "[tensor([[  552,   150,    48,  ...,   203,   105, 12305],\n",
      "        [  581,   127,    35,  ...,   694,   293,  8387],\n",
      "        [   79,   653,    24,  ...,  1531,   534,  6537],\n",
      "        ...,\n",
      "        [  257,   542,    41,  ...,   851,   317,  7797],\n",
      "        [  557,   134,    43,  ...,   682,   246,  7923],\n",
      "        [  604,   123,    28,  ...,  1820,   469,  7999]]), tensor([[3841],\n",
      "        [2083],\n",
      "        [ 926],\n",
      "        ...,\n",
      "        [1441],\n",
      "        [2871],\n",
      "        [1532]])] [tensor([[  415,   402,    42,  ...,  1896,   443,   583],\n",
      "        [  569,   160,    35,  ...,  1474,   543,  1505],\n",
      "        [  205,   494,    24,  ...,   910,   323, 11324],\n",
      "        ...,\n",
      "        [  562,   143,    47,  ...,   792,   311,  4681],\n",
      "        [  464,   382,    20,  ...,  1693,   514,  3165],\n",
      "        [  676,    23,    34,  ...,   668,   280,  6492]]), tensor([[  62],\n",
      "        [1960],\n",
      "        [2525],\n",
      "        ...,\n",
      "        [ 945],\n",
      "        [ 242],\n",
      "        [1252]])]\n",
      "Fold((3,)), train: 16512, test: 4128\n",
      "[tensor([[  160,   484,    19,  ...,  1085,   596,  6110],\n",
      "        [  581,   161,    18,  ...,   646,   211,  2186],\n",
      "        [  563,   126,    26,  ...,  1471,   518,  9649],\n",
      "        ...,\n",
      "        [  667,    43,    24,  ...,  1061,   470, 11333],\n",
      "        [  628,   111,    14,  ...,  1712,   484, 11614],\n",
      "        [  422,   187,    17,  ...,   667,   305,  6274]]), tensor([[2398],\n",
      "        [1272],\n",
      "        [2253],\n",
      "        ...,\n",
      "        [3841],\n",
      "        [2095],\n",
      "        [2126]])] [tensor([[ 425,  462,   12,  ...,  898,  346, 4801],\n",
      "        [ 573,  123,   21,  ..., 1163,  398, 1655],\n",
      "        [ 586,  136,   41,  ...,  749,  227, 6707],\n",
      "        ...,\n",
      "        [ 627,  152,   38,  ..., 1462,  335,  826],\n",
      "        [ 362,  529,   10,  ..., 1019,  386, 6609],\n",
      "        [ 473,  164,   26,  ...,  755,  221, 7541]]), tensor([[ 783],\n",
      "        [1022],\n",
      "        [1211],\n",
      "        ...,\n",
      "        [ 503],\n",
      "        [ 981],\n",
      "        [1438]])]\n",
      "Fold((4,)), train: 16512, test: 4128\n",
      "[tensor([[  175,   442,    16,  ...,   564,   234, 10140],\n",
      "        [  660,   133,    13,  ...,   974,   310,  9159],\n",
      "        [  394,   338,    22,  ...,   175,    54,     0],\n",
      "        ...,\n",
      "        [  425,   359,    25,  ...,   243,   115,  1187],\n",
      "        [  552,   135,    30,  ...,   650,   330, 11924],\n",
      "        [  261,   543,    13,  ...,  1044,   368,  5765]]), tensor([[2406],\n",
      "        [1197],\n",
      "        [ 585],\n",
      "        ...,\n",
      "        [ 793],\n",
      "        [3841],\n",
      "        [1988]])] [tensor([[  146,   526,    21,  ...,  2406,   955, 12054],\n",
      "        [  590,   156,    30,  ...,  1446,   519,  6835],\n",
      "        [  492,   277,    37,  ...,  1176,   372,   987],\n",
      "        ...,\n",
      "        [  295,   498,    21,  ...,   172,    85,  2829],\n",
      "        [  348,   211,    26,  ...,  1517,   526,  1538],\n",
      "        [  571,   142,    35,  ...,  1683,   467,  1481]]), tensor([[3602],\n",
      "        [1821],\n",
      "        [ 194],\n",
      "        ...,\n",
      "        [2258],\n",
      "        [ 937],\n",
      "        [1226]])]\n",
      "Fold((5,)), train: 16512, test: 4128\n",
      "[tensor([[ 564,  138,   39,  ..., 1589,  432, 4078],\n",
      "        [ 662,  156,   27,  ..., 2349,  654,  682],\n",
      "        [ 545,  167,   24,  ..., 2044,  648, 5505],\n",
      "        ...,\n",
      "        [ 148,  505,   44,  ..., 1293,  692, 4883],\n",
      "        [ 580,  152,   16,  ..., 1563,  567,  259],\n",
      "        [ 685,   11,   16,  ..., 1460,  693, 3124]]), tensor([[ 787],\n",
      "        [ 280],\n",
      "        [1558],\n",
      "        ...,\n",
      "        [2843],\n",
      "        [1423],\n",
      "        [1054]])] [tensor([[ 601,  129,   35,  ...,  612,  195, 8933],\n",
      "        [ 549,  172,   33,  ...,  922,  211, 7184],\n",
      "        [ 297,  488,   22,  ..., 2405,  815, 1580],\n",
      "        ...,\n",
      "        [ 160,  522,   45,  ...,  750,  321, 7220],\n",
      "        [ 581,  129,   44,  ...,  770,  292, 8719],\n",
      "        [ 160,  483,   33,  ..., 2345, 1409, 5747]]), tensor([[1377],\n",
      "        [1235],\n",
      "        [ 519],\n",
      "        ...,\n",
      "        [1010],\n",
      "        [1810],\n",
      "        [3841]])]\n"
     ]
    }
   ],
   "source": [
    "for train_dl, test_dl in dataset.data_loaders():\n",
    "    print(next(iter(train_dl)), next(iter(test_dl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  529,   165,    32,  ...,  1396,   469, 10207],\n",
       "         [  684,    36,    15,  ...,   787,   290, 11128],\n",
       "         [  185,   495,    25,  ...,  1374,   606,  3480],\n",
       "         ...,\n",
       "         [  595,   119,    21,  ...,  2351,   659, 11533],\n",
       "         [  285,   616,    40,  ...,  1653,   709,  3520],\n",
       "         [  463,   361,    26,  ...,  1151,   308,   555]]),\n",
       " tensor([[2064],\n",
       "         [2127],\n",
       "         [1425],\n",
       "         ...,\n",
       "         [2413],\n",
       "         [ 974],\n",
       "         [  75]])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_dl))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = batch[0]\n",
    "torch.chunk(inputs, 8, dim=-1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_tuple = torch.chunk(inputs, 8, dim=-1)\n",
    "ind_out = [input_i for i, input_i in enumerate(inputs_tuple)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_out = torch.stack(ind_out, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(stacked_out, dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, torch.Size([1024, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape[-1], batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nam = NAM(config=config, name=\"NAMModel\", num_inputs=batch[0].shape[-1], num_units=64, shallow=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FeatureNN(\n",
       "   (relu): ReLU(inplace=True)\n",
       "   (dropout): Dropout(p=0.0, inplace=False)\n",
       "   (_h1): Linear(in_features=64, out_features=64, bias=True)\n",
       "   (_h2): Linear(in_features=64, out_features=32, bias=True)\n",
       "   (linear): Linear(in_features=32, out_features=1, bias=True)\n",
       "   (hidden_layers): Sequential(\n",
       "     (0): ExU()\n",
       "     (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "     (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "   )\n",
       " ),\n",
       " FeatureNN(\n",
       "   (relu): ReLU(inplace=True)\n",
       "   (dropout): Dropout(p=0.0, inplace=False)\n",
       "   (_h1): Linear(in_features=64, out_features=64, bias=True)\n",
       "   (_h2): Linear(in_features=64, out_features=32, bias=True)\n",
       "   (linear): Linear(in_features=32, out_features=1, bias=True)\n",
       "   (hidden_layers): Sequential(\n",
       "     (0): ExU()\n",
       "     (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "     (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "   )\n",
       " ),\n",
       " FeatureNN(\n",
       "   (relu): ReLU(inplace=True)\n",
       "   (dropout): Dropout(p=0.0, inplace=False)\n",
       "   (_h1): Linear(in_features=64, out_features=64, bias=True)\n",
       "   (_h2): Linear(in_features=64, out_features=32, bias=True)\n",
       "   (linear): Linear(in_features=32, out_features=1, bias=True)\n",
       "   (hidden_layers): Sequential(\n",
       "     (0): ExU()\n",
       "     (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "     (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "   )\n",
       " ),\n",
       " FeatureNN(\n",
       "   (relu): ReLU(inplace=True)\n",
       "   (dropout): Dropout(p=0.0, inplace=False)\n",
       "   (_h1): Linear(in_features=64, out_features=64, bias=True)\n",
       "   (_h2): Linear(in_features=64, out_features=32, bias=True)\n",
       "   (linear): Linear(in_features=32, out_features=1, bias=True)\n",
       "   (hidden_layers): Sequential(\n",
       "     (0): ExU()\n",
       "     (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "     (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "   )\n",
       " ),\n",
       " FeatureNN(\n",
       "   (relu): ReLU(inplace=True)\n",
       "   (dropout): Dropout(p=0.0, inplace=False)\n",
       "   (_h1): Linear(in_features=64, out_features=64, bias=True)\n",
       "   (_h2): Linear(in_features=64, out_features=32, bias=True)\n",
       "   (linear): Linear(in_features=32, out_features=1, bias=True)\n",
       "   (hidden_layers): Sequential(\n",
       "     (0): ExU()\n",
       "     (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "     (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "   )\n",
       " ),\n",
       " FeatureNN(\n",
       "   (relu): ReLU(inplace=True)\n",
       "   (dropout): Dropout(p=0.0, inplace=False)\n",
       "   (_h1): Linear(in_features=64, out_features=64, bias=True)\n",
       "   (_h2): Linear(in_features=64, out_features=32, bias=True)\n",
       "   (linear): Linear(in_features=32, out_features=1, bias=True)\n",
       "   (hidden_layers): Sequential(\n",
       "     (0): ExU()\n",
       "     (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "     (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "   )\n",
       " ),\n",
       " FeatureNN(\n",
       "   (relu): ReLU(inplace=True)\n",
       "   (dropout): Dropout(p=0.0, inplace=False)\n",
       "   (_h1): Linear(in_features=64, out_features=64, bias=True)\n",
       "   (_h2): Linear(in_features=64, out_features=32, bias=True)\n",
       "   (linear): Linear(in_features=32, out_features=1, bias=True)\n",
       "   (hidden_layers): Sequential(\n",
       "     (0): ExU()\n",
       "     (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "     (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "   )\n",
       " ),\n",
       " FeatureNN(\n",
       "   (relu): ReLU(inplace=True)\n",
       "   (dropout): Dropout(p=0.0, inplace=False)\n",
       "   (_h1): Linear(in_features=64, out_features=64, bias=True)\n",
       "   (_h2): Linear(in_features=64, out_features=32, bias=True)\n",
       "   (linear): Linear(in_features=32, out_features=1, bias=True)\n",
       "   (hidden_layers): Sequential(\n",
       "     (0): ExU()\n",
       "     (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "     (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "   )\n",
       " )]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nam.feature_nns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7615.4722, -8860.6367, -2092.2812,  ..., -8710.9600, -1141.4934,\n",
       "          300.8806], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nam(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn = FeatureNN(config=config, name=\"Feature_NN\", input_shape=batch[0].shape, num_units=64, shallow=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureNN(\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (_h1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (_h2): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (linear): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (hidden_layers): Sequential(\n",
       "    (0): ExU()\n",
       "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-20471.6133,  -8078.4175, -13975.0645,  ...,  37003.4414,\n",
       "          7193.8286,   9193.8447], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnn(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
